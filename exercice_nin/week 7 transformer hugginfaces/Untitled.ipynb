{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9352b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab98df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf604ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch] -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da256688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28fdc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ludovic.souquet\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54313541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9977988600730896}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"illyes loves naruto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "718bae97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b581591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to develop a fully functional and practical application for writing, visualizing, managing, maintaining and creating your own online apps using React. The course is about how to write something which in theory would be as easy'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Generation\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00576f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'les chats de nos grands-parents.\\nUn des petits plus de ce livre de recettes? Son mode de cuisson : un fond de moules accompagn√© d'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"benjamin/gpt2-wechsel-french\")\n",
    "generator(\n",
    "    \"les chats\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5cd717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19619695842266083,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.040526971220970154,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f6be58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\ludovic.souquet\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.38135758,\n",
       "  'word': 'robe',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.4991569,\n",
       "  'word': 'mexi',\n",
       "  'start': 17,\n",
       "  'end': 21}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"roberto lives in mexico.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f9def05",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e42b0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'The market was very volatile today due to the unexpected interest rate hike.',\n",
       " 'labels': ['economy', 'science', 'health'],\n",
       " 'scores': [0.9698341488838196, 0.00038923960528336465, 3.376215317985043e-05]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The market was very volatile today due to the unexpected interest rate hike.\"\n",
    "\n",
    "candidate_labels = ['science', 'economy','health']\n",
    "\n",
    "classifier(text, candidate_labels, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6771b06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"I don't like the food.\",\n",
       " 'labels': ['unsatisfied', 'neutral', 'satisfied'],\n",
       " 'scores': [0.9968516826629639, 0.000567193899769336, 0.00014923184062354267]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I don't like the food.\"\n",
    "\n",
    "candidate_labels = ['satisfied', 'unsatisfied','neutral']\n",
    "hypothesis_template = \"The text show the notion of {} sentiment.\"\n",
    "\n",
    "classifier(text, candidate_labels, multi_label=True, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b34085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f5c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"The new fitness program includes routines that improve cardiovascular health.\"\n",
      "Predicted Category: Economy\n"
     ]
    }
   ],
   "source": [
    "def classify_new_text(new_text):\n",
    "    generator = pipeline('text-generation', model='distilgpt2')\n",
    "    set_seed(42)\n",
    "\n",
    "    prompt_template = \"\"\"The following are examples of text classification:\n",
    "    Text: \"The market was very volatile today due to the unexpected interest rate hike.\"\n",
    "    Category: Economy\n",
    "\n",
    "    Text: \"The new fitness program includes routines that improve cardiovascular health.\"\n",
    "    Category: Health\n",
    "\n",
    "    Text: \"A groundbreaking discovery in renewable energy has been announced.\"\n",
    "    Category: Science\n",
    "\n",
    "    Text: \"The local sports team won their game last night in a surprising upset.\"\n",
    "    Category: Sports\n",
    "\n",
    "    Text: \"{}\"\n",
    "    Category:\"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(new_text)\n",
    "\n",
    "    response = generator(prompt, max_length=200, num_return_sequences=1, temperature=0.7)\n",
    "    generated_text = response[0]['generated_text']\n",
    "\n",
    "    # Attempt to extract the category from the generated text\n",
    "    try:\n",
    "        # Splitting the generated text to find the category part\n",
    "        category_part = generated_text.split(\"Category:\")[1].strip()\n",
    "        # Assuming the category is the first word/phrase followed by any newline or extra text\n",
    "        predicted_category = category_part.split('\\n')[0].strip()\n",
    "        print(f'Text: \"{new_text}\"\\nPredicted Category: {predicted_category}')\n",
    "    except IndexError:\n",
    "        # If the expected format isn't found\n",
    "        print(\"Failed to extract the category. Please check the generated text format.\")\n",
    "\n",
    "# Example usage\n",
    "new_text = \"The new fitness program includes routines that improve cardiovascular health.\"\n",
    "classify_new_text(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "609da8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: setfit in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (1.0.3)\n",
      "Requirement already satisfied: datasets>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from setfit) (2.12.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.1 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from setfit) (2.3.1)\n",
      "Requirement already satisfied: evaluate>=0.3.0 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from setfit) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from setfit) (0.20.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from setfit) (1.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from setfit) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from datasets>=2.3.0->setfit) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from datasets>=2.3.0->setfit) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.3.0->setfit) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->setfit) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.13.0->setfit) (4.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers>=2.2.1->setfit) (4.37.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers>=2.2.1->setfit) (2.2.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.2.1->setfit) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.2.1->setfit) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers>=2.2.1->setfit) (0.1.99)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.2.1->setfit) (10.0.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->setfit) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->setfit) (2.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.3.0->setfit) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.3.0->setfit) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.3.0->setfit) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.3.0->setfit) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.3.0->setfit) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.3.0->setfit) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.3.0->setfit) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2023.11.17)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from responses<0.19->datasets>=2.3.0->setfit) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets>=2.3.0->setfit) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->setfit) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->setfit) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ludovic.souquet\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->setfit) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=2.2.1->setfit) (8.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=2.3.0->setfit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=2.3.0->setfit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=2.3.0->setfit) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install setfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331f6411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ludovic.souquet\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe24d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2051448715.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    datasets-cli clean-cache\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('SetFit/SentEval-CR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c09236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To simulate a real-world scenario with just a few labeled examples, we'll sample 8 examples per class from the training set:\n",
    "train_ds = dataset[\"train\"].shuffle(seed=42).select(range(8 * 2))\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579889d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
